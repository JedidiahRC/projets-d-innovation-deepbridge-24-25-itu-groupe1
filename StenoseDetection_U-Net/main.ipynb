{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afba1920-1eb7-4f82-8086-b921036178d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c0b72e-cbe5-4141-b399-aebb50a268b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 05:46:34.975174: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-03 05:46:35.029773: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-03 05:46:36.571073: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import configparser\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4987fa-bfe1-46bf-9ae7-77092831ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "OUTPUT_DIR = config[\"directory\"][\"result\"]\n",
    "IMG_WIDTH = int(config[\"image\"][\"width\"])\n",
    "IMG_HEIGHT = int(config[\"image\"][\"height\"])\n",
    "CLASS_WEIGHT = float(config[\"model\"][\"class_weight\"])\n",
    "THRESHOLD = float(config[\"model\"][\"threshold\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e645047-0cc3-451f-a853-71f9a6ef65ad",
   "metadata": {},
   "source": [
    "# Détection Carotid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "852f9968-d6b7-4043-b15c-fb3903cd7687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle carotide_detector_v2.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 05:46:37.352173: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement du modèle: 'function' object has no attribute 'replace'\n",
      "Modèle chargé avec succès (méthode alternative)!\n",
      "Traitement de 28 images...\n",
      "[1/28] Traitement de carotide22.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step\n",
      "Prédiction pour carotide22.png sauvegardée dans result\n",
      "[2/28] Traitement de carotide5.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Prédiction pour carotide5.png sauvegardée dans result\n",
      "[3/28] Traitement de carotide4.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Prédiction pour carotide4.png sauvegardée dans result\n",
      "[4/28] Traitement de carotide23.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Prédiction pour carotide23.png sauvegardée dans result\n",
      "[5/28] Traitement de carotide21.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "Prédiction pour carotide21.png sauvegardée dans result\n",
      "[6/28] Traitement de carotide6.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "Prédiction pour carotide6.png sauvegardée dans result\n",
      "[7/28] Traitement de carotide7.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "Prédiction pour carotide7.png sauvegardée dans result\n",
      "[8/28] Traitement de carotide20.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Prédiction pour carotide20.png sauvegardée dans result\n",
      "[9/28] Traitement de carotide24.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "Prédiction pour carotide24.png sauvegardée dans result\n",
      "[10/28] Traitement de carotide30.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Prédiction pour carotide30.png sauvegardée dans result\n",
      "[11/28] Traitement de carotide18.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Prédiction pour carotide18.png sauvegardée dans result\n",
      "[12/28] Traitement de carotide3.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Prédiction pour carotide3.png sauvegardée dans result\n",
      "[13/28] Traitement de carotide2.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Prédiction pour carotide2.png sauvegardée dans result\n",
      "[14/28] Traitement de carotide19.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Prédiction pour carotide19.png sauvegardée dans result\n",
      "[15/28] Traitement de carotide27.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Prédiction pour carotide27.png sauvegardée dans result\n",
      "[16/28] Traitement de carotide26.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Prédiction pour carotide26.png sauvegardée dans result\n",
      "[17/28] Traitement de carotide17.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Prédiction pour carotide17.png sauvegardée dans result\n",
      "[18/28] Traitement de carotide16.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Prédiction pour carotide16.png sauvegardée dans result\n",
      "[19/28] Traitement de carotide28.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Prédiction pour carotide28.png sauvegardée dans result\n",
      "[20/28] Traitement de carotide14.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "Prédiction pour carotide14.png sauvegardée dans result\n",
      "[21/28] Traitement de carotide15.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Prédiction pour carotide15.png sauvegardée dans result\n",
      "[22/28] Traitement de carotide29.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "Prédiction pour carotide29.png sauvegardée dans result\n",
      "[23/28] Traitement de carotide11.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "Prédiction pour carotide11.png sauvegardée dans result\n",
      "[24/28] Traitement de carotide10.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Prédiction pour carotide10.png sauvegardée dans result\n",
      "[25/28] Traitement de carotide12.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Prédiction pour carotide12.png sauvegardée dans result\n",
      "[26/28] Traitement de carotide9.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "Prédiction pour carotide9.png sauvegardée dans result\n",
      "[27/28] Traitement de carotide8.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Prédiction pour carotide8.png sauvegardée dans result\n",
      "[28/28] Traitement de carotide13.png...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Prédiction pour carotide13.png sauvegardée dans result\n",
      "Toutes les prédictions ont été sauvegardées dans result\n"
     ]
    }
   ],
   "source": [
    "# Fonction de perte personnalisée compatible avec différentes versions de TensorFlow\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    # Poids pour les pixels positifs (carotides)\n",
    "    pos_weight = CLASS_WEIGHT\n",
    "    \n",
    "    # Calculer BCE manuellement pour éviter les problèmes de version\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "    bce = -(y_true * tf.math.log(y_pred) + (1.0 - y_true) * tf.math.log(1.0 - y_pred))\n",
    "    \n",
    "    # Appliquer les poids\n",
    "    weighted_bce = bce * (y_true * pos_weight + (1.0 - y_true))\n",
    "    \n",
    "    # Retourner la moyenne\n",
    "    return tf.reduce_mean(weighted_bce)\n",
    "\n",
    "# Fonction Dice comme métrique\n",
    "def dice_coefficient(y_true, y_pred, smooth=1.0):\n",
    "    y_true_f = tf.cast(tf.keras.backend.flatten(y_true), tf.float32)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "    \n",
    "def load_trained_model(model_path):\n",
    "    \"\"\"\n",
    "    Charge un modèle pré-entraîné\n",
    "    \n",
    "    Args:\n",
    "        model_path: Chemin vers le fichier du modèle (.h5)\n",
    "        \n",
    "    Returns:\n",
    "        Le modèle chargé\n",
    "    \"\"\"\n",
    "    print(f\"Chargement du modèle {model_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Charger le modèle avec les fonctions personnalisées\n",
    "        model = load_model(model_path, custom_objects={\n",
    "            'weighted_binary_crossentropy': weighted_binary_crossentropy,\n",
    "            'dice_coefficient': dice_coefficient\n",
    "        })\n",
    "        print(\"Modèle chargé avec succès!\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement du modèle: {e}\")\n",
    "        \n",
    "        # Essayer une autre méthode de chargement si la première échoue\n",
    "        try:\n",
    "            model = load_model(model_path, compile=False)\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=0.0001),\n",
    "                loss=weighted_binary_crossentropy,\n",
    "                metrics=['accuracy', dice_coefficient]\n",
    "            )\n",
    "            print(\"Modèle chargé avec succès (méthode alternative)!\")\n",
    "            return model\n",
    "        except Exception as e2:\n",
    "            print(f\"Échec de chargement du modèle: {e2}\")\n",
    "            return None\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Prétraite une image pour la prédiction\n",
    "    \n",
    "    Args:\n",
    "        image_path: Chemin vers l'image à prétraiter\n",
    "        \n",
    "    Returns:\n",
    "        L'image prétraitée, redimensionnée et normalisée\n",
    "    \"\"\"\n",
    "    # Charger l'image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"Erreur: Impossible de charger l'image {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Redimensionner\n",
    "    image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    \n",
    "    # Normaliser\n",
    "    image = image / 255.0\n",
    "    \n",
    "    # Amélioration du contraste (CLAHE)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    image = clahe.apply((image * 255).astype(np.uint8)) / 255.0\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Fonction pour superposer un masque sur une image\n",
    "def overlay_mask(image, mask, alpha=0.7):\n",
    "    # Convertir l'image en RGB\n",
    "    if len(image.shape) == 2 or image.shape[2] == 1:\n",
    "        image_rgb = cv2.cvtColor((image * 255).astype(np.uint8).reshape(IMG_HEIGHT, IMG_WIDTH), cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        image_rgb = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Créer une superposition rouge pour les carotides\n",
    "    overlay = image_rgb.copy()\n",
    "    binary_mask = (mask > THRESHOLD).astype(np.uint8)\n",
    "    \n",
    "    # Assurer que le masque a la bonne forme\n",
    "    if len(binary_mask.shape) == 3 and binary_mask.shape[2] == 1:\n",
    "        binary_mask = binary_mask.reshape(IMG_HEIGHT, IMG_WIDTH)\n",
    "    \n",
    "    overlay[binary_mask > 0] = [255, 0, 0]  # Rouge\n",
    "    \n",
    "    # Mélanger l'image originale et la superposition\n",
    "    blended = cv2.addWeighted(image_rgb, 1 - alpha, overlay, alpha, 0)\n",
    "    \n",
    "    return blended\n",
    "    \n",
    "def save_prediction_overlay(image, pred_mask, filename):\n",
    "    overlay = overlay_mask(image, pred_mask)\n",
    "    cv2.imwrite(filename, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "def predict_on_image(model, image_path, output_dir):\n",
    "    \"\"\"\n",
    "    Fait une prédiction sur une seule image\n",
    "    \n",
    "    Args:\n",
    "        model: Modèle pré-entraîné\n",
    "        image_path: Chemin vers l'image à prédire\n",
    "        output_dir: Répertoire où sauvegarder les résultats\n",
    "    \"\"\"\n",
    "    # Créer le répertoire de sortie s'il n'existe pas\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"mask\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"overlay\"), exist_ok=True)\n",
    "    \n",
    "    # Prétraiter l'image\n",
    "    image = preprocess_image(image_path)\n",
    "    \n",
    "    if image is None:\n",
    "        return\n",
    "    \n",
    "    # Préparer l'image pour la prédiction\n",
    "    input_image = image.reshape(1, IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "    \n",
    "    # Faire la prédiction\n",
    "    prediction = model.predict(input_image)[0]\n",
    "    \n",
    "    # Binariser la prédiction\n",
    "    binary_pred = (prediction > THRESHOLD).astype(np.uint8)\n",
    "    \n",
    "    # Extraire le nom de base du fichier\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    \n",
    "    # Sauvegarder le masque prédit\n",
    "    mask_path = os.path.join(output_dir, \"mask\", f\"{base_name}_mask.png\")\n",
    "    cv2.imwrite(mask_path, binary_pred.reshape(IMG_HEIGHT, IMG_WIDTH) * 255)\n",
    "    \n",
    "    # Sauvegarder l'overlay\n",
    "    overlay_path = os.path.join(output_dir, \"overlay\",f\"{base_name}_overlay.png\")\n",
    "    save_prediction_overlay(image, binary_pred, overlay_path)\n",
    "    \n",
    "    print(f\"Prédiction pour {os.path.basename(image_path)} sauvegardée dans {output_dir}\")\n",
    "    \n",
    "def predict_on_directory(model, input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Fait des prédictions sur toutes les images d'un répertoire\n",
    "    \n",
    "    Args:\n",
    "        model: Modèle pré-entraîné\n",
    "        input_dir: Répertoire contenant les images à prédire\n",
    "        output_dir: Répertoire où sauvegarder les résultats\n",
    "    \"\"\"\n",
    "    # Créer le répertoire de sortie s'il n'existe pas\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Lister toutes les images du répertoire\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.tif', '*.tiff', '*.bmp']\n",
    "    image_paths = []\n",
    "    for ext in image_extensions:\n",
    "        image_paths.extend(glob(os.path.join(input_dir, ext)))\n",
    "        image_paths.extend(glob(os.path.join(input_dir, ext.upper())))\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(f\"Aucune image trouvée dans {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Traitement de {len(image_paths)} images...\")\n",
    "    \n",
    "    # Faire les prédictions sur chaque image\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(f\"[{i+1}/{len(image_paths)}] Traitement de {os.path.basename(image_path)}...\")\n",
    "        predict_on_image(model, image_path, output_dir)\n",
    "    \n",
    "    print(f\"Toutes les prédictions ont été sauvegardées dans {output_dir}\")\n",
    "\n",
    "def detection_carotide(model_path, dir):\n",
    "    \"\"\"\n",
    "    Mode test : utilise un modèle pré-entraîné pour faire des prédictions\n",
    "    \n",
    "    Args:\n",
    "        model_path: Chemin vers le fichier du modèle (.h5)\n",
    "        dir: Répertoire contenant les images\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vérifier que le modèle existe\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Erreur: Le modèle {model_path} n'existe pas.\")\n",
    "        return\n",
    "    \n",
    "    # Vérifier que le répertoire de test existe\n",
    "    if not os.path.exists(dir):\n",
    "        print(f\"Erreur: Le répertoire de test {dir} n'existe pas.\")\n",
    "        return\n",
    "    \n",
    "    # Charger le modèle\n",
    "    model = load_trained_model(model_path)\n",
    "    \n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # Créer un répertoire pour les résultats\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Faire des prédictions sur toutes les images du répertoire de test\n",
    "    predict_on_directory(model, dir, OUTPUT_DIR)\n",
    "\n",
    "detection_carotide(\"carotide_detector_v2.h5\", \"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0cddb-911e-43b3-891f-67c15ef58427",
   "metadata": {},
   "source": [
    "# Détection stenose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509312b3-a2d9-4727-84c3-f78fab3badaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(OUTPUT_DIR, \"mask\", \"*.png\"))\n",
    "carotid_left = []\n",
    "carotid_right = []\n",
    "for f in files :\n",
    "    image = cv2.imread(f, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # On transforme en noir/blanc pur : 0 ou 255\n",
    "    _, thresh = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    shapes = []\n",
    "    \n",
    "    for i, cnt in enumerate(contours):\n",
    "        area = cv2.contourArea(cnt)\n",
    "        M = cv2.moments(cnt)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])  # Coordonnée x du centre\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])  # Coordonnée y du centre\n",
    "            shapes.append({\"id\": i+1, \"area\": area, \"cx\": cx, \"cy\": cy, \"contour\": cnt})\n",
    "    \n",
    "    shapes.sort(key=lambda s: s[\"cx\"])\n",
    "    \n",
    "    left = shapes[0]\n",
    "    right = shapes[1]\n",
    "    carotid_left.append(left[\"area\"])\n",
    "    carotid_right.append(right[\"area\"])\n",
    "\n",
    "# # 5. Afficher les contours sur l'image originale\n",
    "# output = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "# cv2.drawContours(output, contours, -1, (0, 0, 255), 2)\n",
    "\n",
    "# plt.imshow(output[..., ::-1])\n",
    "# plt.title(\"Contours détectés\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e555ac14-2648-4dc2-b8f5-4ecf59e21846",
   "metadata": {},
   "source": [
    "### Sténose estimée (moyenne pondérée)\n",
    "\n",
    "La sténose estimée se calcule par :\n",
    "\n",
    "$$\n",
    "\\text{Sténose estimée (\\%)} = \n",
    "\\frac{\\sum_i w_i \\cdot \\left( 1 - \\frac{A_i}{A_{\\max}} \\right)}{\\sum_i w_i} \\times 100\n",
    "$$\n",
    "\n",
    "Où :  \n",
    "\n",
    "- $A_{\\max}$ = aire maximale (approximation du diamètre normal)  \n",
    "- $A_i$ = aire détectée sur l’image $i$  \n",
    "- $w_i$ = poids de chaque image (par exemple $1$ si toutes les images ont le même poids, ou selon qualité de segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72fc0f6-dc2f-4fcd-8e03-d894e91b5943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sténose pondérée carotid gauche : 46.47%\n",
      "Sténose pondérée carotid droite : 64.88%\n"
     ]
    }
   ],
   "source": [
    "A_left_max = max(carotid_left)\n",
    "A_right_max = max(carotid_right)\n",
    "\n",
    "# Calcul sténose pondérée (poids = 1 ici)\n",
    "stenosis_left = np.mean([1 - np.sqrt(a / A_left_max) for a in carotid_left]) * 100\n",
    "stenosis_right = np.mean([1 - np.sqrt(a / A_right_max) for a in carotid_right]) * 100\n",
    "\n",
    "print(f\"Sténose pondérée carotid gauche : {stenosis_left:.2f}%\")\n",
    "print(f\"Sténose pondérée carotid droite : {stenosis_right:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
